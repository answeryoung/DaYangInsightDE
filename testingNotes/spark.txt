cdSpark
sh sbin/start-all.sh
# you should see messages about all the works' ips
# I have webui-port:8080 spark://$sparkMasterIp:7077
# http://ec2-$sparkMasterIps_dashed.$aws_region_dashed.compute.amazonaws.com:8080/
# link will work if the 8080 port is open to your ip
# It displys all the workers on your cluster

spark-shell
# http://ec2-$sparkMasterIps_dashed.$aws_region_dashed.compute.amazonaws.com:4040/
should be up

sc.setLogLevel("DEBUG")
## "ALL" "DEBUG" "ERROR" "FATAL" "TRACE" "WARN" "INFO" "OFF"
sc.version
sc.sparkUser

val x = sc.parallelize(1 to 1000)
val textFile = sc.textFile("README.md") 
val linesWithSpark = textFile.filter(line => line.contains("Spark"))
linesWithSpark.toDebugString


pyspark --packages \
  org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.6,\
org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.6

# It is important not to leave any spaces between packages
spark-submit --packages \
org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.6\
org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.6 ~/src/test_spark.py


>>> import sys 
>>> from pyspark import SparkContext
>>> from pyspark.streaming import StreamingContext
>>> from pyspark.streaming.kafka import KafkaUtils
>>> brokers = '10.0.0.7:9092,10.0.0.8:9092,10.0.0.9:9092'
>>> topic = 'ecg-000606' 
>>> sc.stop()
>>> sc = SparkContext(appName="JRS")
>>> ssc = StreamingContext(sc, 2)
>>> kvs = KafkaUtils.createDirectStream(ssc, topic, {"metadata.broker.list":brokers})
>>> lines = kvs.map(lambda x: x[1])


https://stackoverflow.com/questions/45392990/spark-streaming-to-data-frame-in-pyspark-json-file
from pyspark.sql import SparkSession
from pyspark.sql import SQLContext
from pyspark.sql import Row
import pandas as pd

def check_json(js, col):
    try:
        data = json.loads(js)
        return [data.get(i) for i in col]
    except:
        return []

def convert_json2df(rdd, col):
    ss = SparkSession(rdd.context)
    if rdd.isEmpty():
        return
    df = ss.createDataFrame(rdd, schema=StructType("based on 'col'"))
    df.show()

lines = kvs.map( 
ssc.socketTextStream('localhost', 9999) \
    .map(lambda x: check_json(x, cols)) \
    .filter(lambda x: x) \
    .foreachRDD(lambda x: convert_json2df(x, cols))

    
    
https://stackoverflow.com/questions/51070251/pyspark-explode-json-in-column-to-multiple-columns    
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StructField, StringType

schema = StructType(
    [
        StructField('key1', StringType(), True),
        StructField('key2', StringType(), True)
    ]
)

df.withColumn("data", from_json("data", schema))\
    .select(col('id'), col('point'), col('data.*'))\
    .show()    
    
    
    

    
    
    
    
lines = kvs.map(lambda x: x) \
    .foreachRDD(lambda x: convert_json2df(x, cols))

    
    
from pyspark.sql import SparkSession
from pyspark.sql import SQLContext
from pyspark.sql import Row
from pyspark.sql.functions import explode
from pyspark.sql.functions import split
from pyspark.sql.functions import window
import pandas as pd    
    
brokers = '10.0.0.7:9092,10.0.0.8:9092,10.0.0.9:9092'
cols = ['topic','record_Meta','segment_meta','signal_meta','subject_meta','signal']     
TopicPattern = "ecg-00000*" 
Topic = "ecg-000606"
subscribeType = "subscribe"

spark = SparkSession\
        .builder\
        .appName("StructuredNetworkWordCountWindowed")\
        .getOrCreate()

# Create DataSet representing the stream of input lines from kafka
lines = spark\
       .readStream\
       .format("kafka")\
       .option("kafka.bootstrap.servers", brokers)\
       .option(subscribeType, Topic)\
       .load()\
       .selectExpr("CAST(value AS STRING)")
       
line_struc = lines.select(fn.json_tuple('value', 'display_id', 'ad_id','clicked','uuid','timestamp','document_id','platform','geo_location') \
            .alias('display_id', 'ad_id','clicked','uuid','timestamp','document_id','platform','geo_location'))
       
       
       
       
       # Split the lines into words
# explode turns each item in an array into a separate row
words = lines.select(
        explode(
            split(lines.value, ',')
        ).alias('word')
    )

# Generate running word count
wordCounts = words.groupBy('word').count()    
# Start running the query that prints the running counts to the console
query = wordCounts\
       .writeStream\
       .outputMode('append')\
       .format('console')\
       .start()
query.awaitTermination()





https://spark.apache.org/docs/2.0.2/structured-streaming-kafka-integration.html 
ds3 = spark
  .readStream()
  .format("kafka")
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
  .option("subscribePattern", "topic.*")
  .load()
ds3.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)") 
 